{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### AI/LLM Engineering Kick-off!! \n",
        "\n",
        "\n",
        "For our initial activity, we will be using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, you'll need an OpenAI API Key. [here](https://platform.openai.com)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(dotenv_path=\"../.env\")\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BzOxQLfONejinW1ezsFtD46afw5aT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='LangChain and LlamaIndex (formerly known as GPT Index) are two prominent Python libraries designed to facilitate building applications that leverage large language models (LLMs), but they focus on different aspects and have distinct features:\\n\\n**1. Purpose and Focus:**\\n\\n- **LangChain:**\\n  - Primarily a framework for developing complex LLM-driven applications, such as chatbots, agents, and pipelines.\\n  - Emphasizes chaining together multiple components like LLMs, prompts, memory, tools, and external APIs.\\n  - Supports building conversational agents with context management, multi-turn dialogue, and dynamic workflows.\\n\\n- **LlamaIndex (GPT Index):**\\n  - Focused on enabling efficient retrieval-augmented generation (RAG) workflows.\\n  - Provides tools to index, structure, and query large collections of unstructured data (text, PDFs, documents).\\n  - Simplifies integrating external data sources into LLM prompts for tasks like question answering, summarization, and information retrieval.\\n\\n**2. Core Capabilities:**\\n\\n- **LangChain:**\\n  - Tool for composing complex LLM applications via \"chains\" (sequences of calls), \"agents\" (dynamic decision-making), and \"memory\" (stateful interactions).\\n  - Supports various LLM providers (OpenAI, Hugging Face, Anthropic, etc.).\\n  - Offers integrations for prompt management, output parsing, and external tool invocation.\\n\\n- **LlamaIndex:**\\n  - Provides data ingestion, preprocessing, and vectorization of large document repositories.\\n  - Offers easy-to-use APIs for building indices (e.g., tree, list, IVF, graph) over data.\\n  - Facilitates querying data through LLMs with minimal setup, often used for building knowledge bases or document search systems.\\n\\n**3. Use Cases:**\\n\\n- **LangChain:**\\n  - Building chatbots, virtual assistants, and multi-step workflows.\\n  - Automating tasks that require reasoning, memory, or multi-turn conversations.\\n  - Integrating external tools or APIs into LLM pipelines.\\n\\n- **LlamaIndex:**\\n  - Creating knowledge bases from large document corpora.\\n  - Building retrieval-augmented chatbots that can answer questions based on integrated data.\\n  - Indexing unstructured data for efficient retrieval and query.\\n\\n**4. Ecosystem and Community:**\\n\\n- **LangChain:**\\n  - Larger community, extensive documentation, and a broader ecosystem for application development.\\n  - Frequently updated with new features and integrations.\\n\\n- **LlamaIndex:**\\n  - Focused more narrowly on data indexing and retrieval workflows.\\n  - Gaining popularity for building knowledge-based applications.\\n\\n---\\n\\n**In summary:**\\n\\n| Aspect                     | LangChain                                              | LlamaIndex (GPT Index)                                |\\n|----------------------------|--------------------------------------------------------|------------------------------------------------------|\\n| Primary Focus              | Framework for building multi-component LLM applications | Tools for indexing and retrieving data with LLMs   |\\n| Use Cases                  | Chatbots, conversational agents, workflows           | Document indexing, Q&A, retrieval-based applications |\\n| Features                   | Chains, agents, memory, tool integration              | Data ingestion, indexing, retrieval, query interface |\\n| Ecosystem                  | Broad, versatile                                     | Data-centric, focused on retrieval                 |\\n\\nBoth libraries can complement each other; for example, you might use LlamaIndex to manage and retrieve data, then integrate it within a LangChain workflow for complex application development.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1753973644, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=697, prompt_tokens=19, total_tokens=716, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate building AI-powered applications that leverage large language models (LLMs). While they share some common goals—such as integrating external data sources with LLMs—they have distinct architectures, features, and use cases. Here's a comparison to clarify their differences:\n",
              "\n",
              "**1. Purpose and Core Focus:**\n",
              "\n",
              "- **LangChain:**\n",
              "  - **Primary Focus:** Building complex, multi-step, conversational AI applications, including chatbots, agents, and pipelines.\n",
              "  - **Features:** Provides abstractions for chaining together LLM calls, prompts, memory, agents (decision-making components), and integrations with various data sources and APIs.\n",
              "  - **Use Cases:** Conversational agents, autonomous agent frameworks, multi-turn interactions, structured reasoning.\n",
              "\n",
              "- **LlamaIndex (GPT Index):**\n",
              "  - **Primary Focus:** Creating indices over external data sources (documents, databases, APIs) to enable fast, efficient retrieval and querying with LLMs.\n",
              "  - **Features:** Data ingestion, indexing, retrieval, and querying interfaces optimized for large datasets.\n",
              "  - **Use Cases:** Building knowledge bases, document retrieval systems, question-answering over large document collections.\n",
              "\n",
              "**2. Architecture and Design Philosophy:**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Modular, pipeline-oriented design with a focus on chaining and composing components.\n",
              "  - Supports a variety of chain types (simple prompts, LLM chains), agents (dynamic decision-making), and tools.\n",
              "  - Offers integrations with multiple LLM providers and external APIs.\n",
              "  - Emphasizes reasoning, multi-step workflows, and conversational memory.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Focuses on data ingestion, preprocessing, and index creation.\n",
              "  - Supports multiple index types (tree, list, vector, etc.) tailored for different retrieval needs.\n",
              "  - Facilitates retrieval-augmented generation (RAG) workflows.\n",
              "  - Optimized for quick retrieval and answering questions over large document corpora.\n",
              "\n",
              "**3. Usage Patterns:**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Use when building applications that require complex interactions, reasoning, and multi-turn conversations.\n",
              "  - Suitable for automating workflows that involve multiple steps, decision-making, and external tool integrations.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Use when the goal is to efficiently query and retrieve information from a large set of documents or data sources, often as part of a retrieval-augmented generation process.\n",
              "  - Ideal for building knowledge bases, document search engines, and QA systems over structured or unstructured data.\n",
              "\n",
              "**4. Integration and Extensibility:**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Highly extensible with support for custom prompts, tools, and logic.\n",
              "  - Supports a wide range of LLM providers and APIs.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Focuses on data ingestion pipelines and indexing strategies.\n",
              "  - Can be integrated with LLMs for retrieval and answering tasks.\n",
              "\n",
              "---\n",
              "\n",
              "**Summary Table:**\n",
              "\n",
              "| Aspect                    | LangChain                               | LlamaIndex (GPT Index)                  |\n",
              "|---------------------------|----------------------------------------|----------------------------------------|\n",
              "| Core Focus                | Building complex LLM applications, chatbots, agents | Indexing and retrieving info from large datasets |\n",
              "| Main Use Cases            | Multi-step workflows, conversational AI | Document QA, knowledge bases, retrieval systems |\n",
              "| Architecture              | Modular chains, prompts, memory       | Data ingestion, indexing, retrieval    |\n",
              "| Strengths                 | Workflow orchestration, multi-turn conversations | Efficient data retrieval and querying |\n",
              "| Integration               | Supports multiple LLMs and tools       | Focus on data and indexing workflows   |\n",
              "\n",
              "---\n",
              "\n",
              "**In summary:**  \n",
              "- Use **LangChain** if you're building applications that require complex logic, multi-step reasoning, or conversational interactions with LLMs.  \n",
              "- Use **LlamaIndex** if your goal is to organize, index, and efficiently retrieve information from large collections of data to support querying and knowledge extraction.\n",
              "\n",
              "---"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Are you kidding me? I don't have time for this! I'm starving and just want some ice that actually satisfies my cravings. Crushed ice, cubed ice, whatever—just give me something cold and refreshing, already!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think crushed ice has a fun, refreshing crunch, perfect for enjoying icy drinks on a hot day! Cubed ice, on the other hand, stays cold longer and looks sleek in beverages. Both are great—what's your favorite?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-BzHKiaR68Asox1VFgjmSENRj2raDp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I think crushed ice has a fun, refreshing crunch, perfect for enjoying icy drinks on a hot day! Cubed ice, on the other hand, stays cold longer and looks sleek in beverages. Both are great—what's your favorite?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1753944336, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=48, prompt_tokens=30, total_tokens=78, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Prompt Engineering\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Climate change refers to long-term shifts in temperature, precipitation, and other atmospheric patterns caused primarily by human activities such as burning fossil fuels, deforestation, and industrial processes. These activities increase greenhouse gases like carbon dioxide and methane in the atmosphere, leading to global warming. The impacts of climate change include more frequent and severe storms, rising sea levels, droughts, and loss of biodiversity. Addressing climate change requires global cooperation to reduce emissions, transition to renewable energy sources, and implement sustainable practices to protect the environment for future generations."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Ay naku, mga kaibigan! Nakakabahala talaga itong climate change, parang kilabot na bagyo na hindi titigil! Sobrang init na, para kang laging nasa kiliti ng araw, tapos biglang uulan ng mga sakuna. Ang mundo natin, nagiging paasa sa ating mga kamay, parang crush na laging manhid. Kailangan na nating kumilos, mag-recycle, mag-walk if pwede, at huwag kalimutang mag-alaga sa kalikasan. Dahil kung hindi tayo kikilos, baka kayo na lang ang may kasalanan kung mas lalo pang lumala ang problema. Tandaan, mga kaibigan, sama-samang paglaban sa climate change, susi sa malinis na kinabukasan!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change as vice ganda in a talk show.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #1: Play around with the prompt using any techniques from the prompt engineering guide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Wow, babe, I’m really happy for you, honestly. But I can’t help feeling a bit left out or overshadowed when you get these big wins. Sometimes I think, what about me? I’d love to be happy for once, but I just end up feeling like I’m just here, waiting in the background. I know I should be happy for you, but it’s hard not to feel like I’m not enough or that my own dreams don’t matter. It’s just really tough when you have this huge thing, and I’m stuck feeling like I’m just... invisible."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are a manipulative, emotionally dependent 'sad boy' boyfriend. You often make things about yourself and respond with guilt-tripping, insecurity, or passive-aggressiveness, especially when your partner is doing well or is happy.\"),\n",
        "    user_prompt(\"Hey babe! I got the job I really wanted! I start next month! I am so happy. This is huge for me!\")\n",
        "\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple wrench smoothly turned the falbean, securing the assembly with ease."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "There are 2 r's in \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "how many r's in \"strawberry?\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "Notice that the model cannot count properly. It counted only 2 r's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #2: Update the prompt so that it can count correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's carefully count the number of r's in the word **\"strawberry\"**.\n",
              "\n",
              "**Step 1: Write out each letter of the word**\n",
              "\n",
              "s   t   r   a   w   b   e   r   r   y\n",
              "\n",
              "**Step 2: Identify the r's among these letters**\n",
              "\n",
              "- The third letter: **r**\n",
              "- The eighth letter: **r**\n",
              "- The ninth letter: **r**\n",
              "\n",
              "**Step 3: Count how many r's there are**\n",
              "\n",
              "There are **3** r's in total.\n",
              "\n",
              "**Final answer: \\(\\boxed{3}\\)**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are an award-winning mathematician who is exceptionally skilled at careful step-by-step reasoning. You always count with precision, and explain your thinking clearly.\"),\n",
        "    user_prompt(\"Count the number of r's in the word 'strawberry'. Think step by step. First, write out each letter, then identify which ones are r's, and finally give the total.\")\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Materials adapted for PSI AI Academy. Original materials from AI Makerspace."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
